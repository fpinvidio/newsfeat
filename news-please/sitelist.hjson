# This is a HJSON-File, so comments and so on can be used! See https://hjson.org/
# Furthermore this is first of all the actual config file, but as default just filled with examples.
{
  # Every URL has to be in an array-object in "base_urls".
  # The same URL in combination with the same crawler may only appear once in this array.
  "base_urls" : [
    {
      # nytimes.com should run pretty well with default config:
      "url": "https://www.nytimes.com",
      "crawler": "RecursiveCrawler"

      # to create an additional RssCrawler daemon for this site that runs every hour, we could either use
      # "additional_rss_daemon": 3600
      # or create an additional array-object with "crawler": "RssCrawler" and "daemonize": 3600
      # it is not possible to create an additional_rss_daemon for a daemonized array-object
    }
  ]
}
