{"authors": ["Nate Cohn"], "date_download": "2018-11-16 04:03:18", "date_modify": "2018-11-16 04:03:18", "date_publish": "2018-09-06 20:40:05", "description": "A look at the challenges and trade-offs of conducting a political survey.", "filename": "2018_09_06_upshot_live-poll-explainer_1542340998.html", "image_url": "https://static01.nyt.com/images/2018/09/06/upshot/live-polls-promo/live-polls-promo-facebookJumbo.jpg", "language": "en", "localpath": "/Users/federicoperezinvidio/Projects/illinois/newsfeat/news-please//data/2018/11/16/nytimes.com/2018_09_06_upshot_live-poll-explainer_1542340998.html", "title": "Why Did We Do the Poll the Way We Did?", "title_page": "Why Did We Do the Poll the Way We Did? - The New York Times", "title_rss": "NULL", "source_domain": "nytimes.com", "text": "Over all, the results indicate that people do a pretty good job of predicting their own turnout. Yes, most of the people who say they’re “almost certain to vote” wind up voting. But not all. And yes, most of the people who say they’re “not very likely to vote” don’t turn out, but some do.\nAs a result, we don’t draw a hard line between “likely” and “unlikely” voters. Everyone, in our view, has some probability of voting.\nOne interesting thing that complicates this analysis is that poll respondents are much likelier to vote than nonrespondents, even after controlling for everything else. In other words, if we thought you had a 10 percent chance of voting before you picked up the phone, we instantly think you have a 35 percent chance of voting the moment you agree to take a political survey. That decision alone reveals that you’re really likelier to vote than your demographics and history of voting would suggest. After accounting for this phenomenon, we wind up giving a smaller bonus to unlikely voters who say they’re going to vote, simply because we don’t treat them as if they were all that unlikely to vote in the first place.\nOur analysis of validated turnout in prior Upshot/Siena polls indicates that our models do a much better job of predicting turnout than self-reported intention. If we think you will vote, and you say you won’t, you’re probably still going to vote. But there is still some value in this data: If we weren’t sure you were going to vote, we do care quite a bit about what you told us.\nIt is worth noting, however, that just because this was true in our prior polls doesn’t mean it will be true in the future. Take the August special election in Ohio 12. We ran a test there in late July to work out the kinks on the infrastructure for this project. We didn’t get enough respondents to complete a full poll, but for illustrative purposes, consider the way this would have all played out:\nAlmost certain/already voted: Even\nSelf-report only Balderson+1\nResult: Balderson+1\nWhat our estimate would have been Balderson+4\nMidterm model only Balderson+6\nMaybe it should be no surprise that self-reported turnout fared a little better in a special election than in regularly scheduled elections, where it’s presumably easier to model turnout using historical data.\nOr maybe it’s a harbinger of what’s going to happen this November, and our turnout models won’t be as useful as they’ve been in the past.\nEither way, we’ll show you all of these possibilities on our page. No matter what happens, we hope this is a transparent look into why polling works and why sometimes it does not.", "url": "https://www.nytimes.com/2018/09/06/upshot/live-poll-explainer.html"}