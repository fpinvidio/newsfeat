{"authors": ["Robin Marantz Henig"], "date_download": "2018-11-16 04:07:22", "date_modify": "2018-11-16 04:07:22", "date_publish": "2015-01-09 15:27:50", "description": "Automated machines may soon care for the sick and fight in wars. Can they learn to make moral choices?", "filename": "2015_01_11_magazine_death-by-robot.html_action=click&module=RelatedCoverage&pgtype=Article&region=Footer_1542341242.html", "image_url": "https://static01.nyt.com/images/2015/01/11/magazine/11eureka/11mag-11eureka.t_CA0-facebookJumbo.jpg", "language": "en", "localpath": "/Users/federicoperezinvidio/Projects/illinois/newsfeat/news-please//data/2018/11/16/nytimes.com/2015_01_11_magazine_death-by-robot.html_action=click&module=RelatedCoverage&pgtype=Article&region=Footer_1542341242.html", "title": "Death by Robot", "title_page": "Death by Robot - The New York Times", "title_rss": "NULL", "source_domain": "nytimes.com", "text": "It’s a shorter leap than you might think, technically, from a Roomba vacuum cleaner to a robot that acts as an autonomous home-health aide, and so experts in robot ethics feel a particular urgency about these challenges. The choices that count as “ethical” range from the relatively straightforward — should Fabulon give the painkiller to Sylvia? — to matters of life and death: military robots that have to decide whether to shoot or not to shoot; self-driving cars that have to choose whether to brake or to swerve. These situations can be difficult enough for human minds to wrestle with; when ethicists think through how robots can deal with them, they sometimes get stuck, as we do, between unsatisfactory options.\nAmong the roboticists I spoke to, the favorite example of an ethical, autonomous robot is the driverless car, which is still in the prototype stage at Google and other companies. Wendell Wallach, chairman of the technology-and-ethics study group at Yale’s Interdisciplinary Center for Bioethics, says that driverless cars will no doubt be more consistently safe than cars are now, at least on the highway, where fewer decisions are made and where human drivers are often texting or changing lanes willy-nilly. But in city driving, even negotiating a four-way stop sign might be hard for a robot. “Humans try to game each other a little,” Wallach says. “They rev up the engine, move forward a little, until finally someone says, ‘I’m the one who’s going.’ It brings into play a lot of forms of intelligence.” He paused, then asked, “Will the car be able to play that game?”\nAnd there are far more complex examples than the four-way stop, Wallach says, like situations in which three or four things are happening at once. Let’s say the only way the car can avoid a collision with another car is by hitting a pedestrian. “That’s an ethical decision of what you do there, and it will vary each time it happens,” he says. Is the pedestrian a child? Is the alternative to swerve away from the child and into an S.U.V.? What if the S.U.V. has just one occupant? What if it has six? This kind of reasoning is what the philosopher Patrick Lin, director of the Ethics and Emerging Sciences Group at Cal Poly, calls “moral math.” It evokes the classic Ethics 101 situation known as the trolley problem: deciding whether a conductor should flip a switch that will kill one person to avoid a crash in which five would otherwise die.\nHere’s the difficulty, and it is something unique to a driverless car: If the decision-making algorithm were to always choose the option in which the fewest people die, the car might avoid another car carrying two passengers by running off the road and risking killing just one passenger: its own. Or it might choose to hit a Volvo instead of a Mini Cooper because its occupants are more likely to survive a crash, which means choosing the vehicle that is more dangerous for its owner to plow into. These assessments can be made with lightning speed. The car records data using lasers, radar and cameras mounted on its roof and windshield, and it makes rapid probabilistic predictions based on what the observed objects have been doing. But this is less an engineering question than a philosophical one, which the makers of such a car will have to resolve and, you would assume, bear some legal responsibility for.\nThe military has developed lethal autonomous weapons systems like the cruise missile and is working on a ground robot to either shoot or hold its fire, based on its assessment of the situation within the international rules of war. It would be programmed, for example, to home in on a permissible target — a person who can be identified as an enemy combatant because he is wearing a uniform, say — or to determine that shooting is not permissible, because the target is in a school or a hospital, or has already been wounded.", "url": "https://www.nytimes.com/2015/01/11/magazine/death-by-robot.html?action=click&module=RelatedCoverage&pgtype=Article&region=Footer"}