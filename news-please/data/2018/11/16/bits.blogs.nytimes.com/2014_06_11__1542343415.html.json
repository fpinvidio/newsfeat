{"authors": ["Steve Lohr", "Topics.Nytimes.Com Top Reference Timestopics People L Steve_Lohr Index.Html"], "date_download": "2018-11-16 04:43:35", "date_modify": "2018-11-16 04:43:35", "date_publish": "2014-06-11 19:55:35", "description": "A surge in data and software to find patterns in it, and advances in storage and communication, have transformed the computing industry.", "filename": "2014_06_11__1542343415.html", "image_url": "https://static01.nyt.com/images/2014/06/12/technology/12BRAIN1/BRAIN1-videoSixteenByNine600.jpg", "language": "en", "localpath": "/Users/federicoperezinvidio/Projects/illinois/newsfeat/news-please//data/2018/11/16/bits.blogs.nytimes.com/2014_06_11__1542343415.html", "title": "Intelligence Too Big for a Single Machine", "title_page": "Intelligence Too Big for a Single Machine - The New York Times", "title_rss": "NULL", "source_domain": "nytimes.com", "text": "Photo\nEver since the computer scientist John McCarthy coined the term artificial intelligence in 1955, the field has gone through cycles of boundless optimism and sobering disillusion. Yet until recently, the supercomputer was the go-to operator of machine intelligence — both in science fiction (HAL, in Stanley Kubrick’s “2001: A Space Odyssey”) and in reality (Watson, IBM’s “Jeopardy!” champ).\nBut three forces have transformed that assumption in the last few years: the surge in data of all kinds, rapid progress in software to find patterns and insights in data, and advances in the technology of data processing, storage and communication.\nNow, computing intelligence can be dispersed globally, marshaled and aggregated as necessary, from far-flung data centers in the digital cloud. Google led the way, showing the power of data-driven artificial intelligence delivered over the cloud, not only in search but also in tasks like language translation and computer vision. Artificial intelligence run through the cloud is now the dominant approach used by researchers at technology companies, universities and government labs.\n“We’re seeing a rebirth of artificial intelligence driven by the cloud, huge amounts of data and the learning algorithms of software,” said Larry Smarr, founding director of the California Institute for Telecommunications and Information Technology.\nSpecial Section: Cloud Computing An inside look at how technology is remaking an industry, lowering costs for some and handing even more influence to a handful of powerful companies.\nThe emerging global network, Mr. Smarr said, will be the equivalent of a “planetary computer.” What might that mean, in terms of its practical effect on everyday life?\nMr. Smarr points to the recent movie “Her” as a fairly accurate glimpse of what will be possible in the not-too-distant future. The protagonist, Theodore Twombly (played by Joaquin Phoenix), has clever software on his smartphone that seems to know all about him. It has read his email, his text messages and the books, magazines and everything else he has read. It has seen all the movies he has seen. It knows his buying habits and preferences. It retrieves information and answers at his whim. It communicates with him by talking, conversationally (in the voice of Scarlett Johansson).\n“That’s where we’re headed,” Mr. Smarr said. “That kind of hyper-personalized assistance is going to be common in 10 years. It will appear to be on your smartphone or Google Glass, but it will actually be in the cloud.”\nSome predict that we are headed much further. Ray Kurzweil, an inventor, scientist and futurist, joined Google in 2012 to work on an artificial intelligence project known internally as Google Brain. Mr. Kurzweil has embraced a concept called “the singularity,” which is essentially when computing intelligence surpasses human intelligence — not just on isolated pursuits like playing chess or “Jeopardy!,” but really leaving human intelligence in general in the dust.\nMr. Kurzweil wrote a 2005 book on the subject, “The Singularity Is Near,” and welcomes the prospect, asserting that the supersmart digital intelligence will enrich the life of humans. Others are skeptical, both that it will happen and that it will be a good thing if it does. But in any case, the singularity is a ways off. Mr. Kurzweil puts it at 2045.\nPhoto\nJeff Dean, a research fellow at Google, focuses on accelerating the progress of artificial intelligence in tasks like computer vision and understanding the meaning of words. Until a few years ago, for example, Google image searches were executed mainly by identifying the text labels affixed to pictures. Today, many images are identified by software analyzing the patterns of digital pixels in a picture or video. And, Mr. Dean said, the technology can pick out a leopard in a picture, and know it is not a lion or a cheetah, recognizing the distinctive pixel patterns of various big cats.\nMobilizing the firepower of Google’s large cloud data centers, Mr. Dean said, enables his team to “bring a lot of computation to bear on these kinds of problems.”\nUnderstanding not just words but also their context and meaning is another big challenge. Current search technology does a good job of responding helpfully to a few words, either typed or spoken. So, Mr. Dean noted, when planning a trip in Italy, search engines do well with “train from Rome to Florence” or “hotel in Florence.”\nThe ideal, Mr. Dean explained, would be to tell Google that you want to plan a two-week vacation to Italy. Then the smart technology starts working on the trip. The options it offers are based on its ability to understand both Italy and the traveler, like someone who has volunteered personal information. Maybe you have two young children, want to stay in the country in Tuscany and like to hike. “You kind of want Google to know you,” Mr. Dean said.\nHis team’s advanced artificial intelligence research, known as deep learning, is “loosely inspired by knowledge of how the brain works,” Mr. Dean said. But there are things the human brain does that silicon-based computing still only aspires to. The brain, Mr. Dean noted, is amazingly flexible and efficient, firing up and shutting down memory systems, so that the part of your brain that holds information on English literature or taking out the trash shuts down when you look at a picture of a leopard.\n“We don’t have a great handle on how to build those kinds of dynamically evolving memory systems,” Mr. Dean said. “Google and others are working on that, but it’s really nascent.”\nAt IBM’s laboratories, the researchers emphasize that their mission in developing “cognitive” computing technology is to build systems that can learn from and interact with people rather than try to replace them. Guruduth S. Banavar, director of IBM’s cognitive computing research, said the result should be “way better than either a human or a computer system can do alone.”\nMr. Banavar prefers to think of this smart technology as intelligent augmentation, or I.A., instead of artificial intelligence, or A.I.\nWatson is a pioneering cognitive system that IBM is now retooling for mainstream industries like medicine, finance and customer service. In each field, Mr. Banavar said, Watson will serve as an adviser. This year, IBM set up a separate Watson business unit and said it would invest $1 billion in it.\nWatson itself has changed with the times. It will be a cloud service. “Data is fundamentally distributed today, so you have to be global to leverage information wherever it is,” Mr. Banavar said. “Cloud is the delivery mechanism.”", "url": "https://bits.blogs.nytimes.com/2014/06/11/intelligence-too-big-for-a-single-machine/"}